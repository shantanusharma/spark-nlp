{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/scala213/converting_models_from_212.ipynb)\n",
    "\n",
    "# Converting Spark NLP Scala 2.12 models to Scala 2.13\n",
    "\n",
    "Most models should work out of the box, when loading models across Scala versions. However, if you have models of the following annotators, you will need to do some manual steps.\n",
    "\n",
    "1. `DependencyParserModel`\n",
    "2. `TextMatcherModel`\n",
    "\n",
    "This notebook will guide you step-by-step on how to convert models saved in Scala 2.12 to Scala 2.13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Start Spark NLP Scala 2.12\n",
    "\n",
    "- This feature was introduced in 6.3.2 so make sure you have at least this version.\n",
    "- Let's install and setup Spark NLP (if running it Google Colab)\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3.2\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "print(sparknlp.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct an example pipeline with `DependencyParserModel` and a `TextMatcherModel`. When we save this pipeline with Spark NLP >= 6.3.2, it will be saved in a format that will be compatible with the Scala 2.13 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextMatcherModel entities\n",
    "! echo \"Dependencies\" > entities.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "dependency_conllu download started this may take some time.\n",
      "Approximate size to download 16.7 MB\n",
      "[OK!]\n",
      "dependency_typed_conllu download started this may take some time.\n",
      "Approximate size to download 2.4 MB\n",
      "[OK!]\n",
      "+---------------------------------------------------------------------------------+--------------+\n",
      "|result                                                                           |result        |\n",
      "+---------------------------------------------------------------------------------+--------------+\n",
      "|[ROOT, Dependencies, represents, words, relationships, Sentence, Sentence, words]|[Dependencies]|\n",
      "+---------------------------------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "sentenceDetector = (\n",
    "    SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    ")\n",
    "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "posTagger = (\n",
    "    PerceptronModel.pretrained().setInputCols([\"token\", \"sentence\"]).setOutputCol(\"pos\")\n",
    ")\n",
    "dependencyParser = (\n",
    "    DependencyParserModel.pretrained()\n",
    "    .setInputCols([\"sentence\", \"pos\", \"token\"])\n",
    "    .setOutputCol(\"dependency\")\n",
    ")\n",
    "typedDependencyParser = (\n",
    "    TypedDependencyParserModel.pretrained()\n",
    "    .setInputCols([\"token\", \"pos\", \"dependency\"])\n",
    "    .setOutputCol(\"labdep\")\n",
    ")\n",
    "\n",
    "textMatcher = (\n",
    "    TextMatcher()\n",
    "    .setInputCols([\"sentence\", \"token\"])\n",
    "    .setEntities(\"entities.txt\", ReadAs.TEXT)\n",
    "    .setOutputCol(\"entity\")\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        posTagger,\n",
    "        dependencyParser,\n",
    "        typedDependencyParser,\n",
    "        textMatcher,\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame(\n",
    "    [[\"Dependencies represents relationships betweens words in a Sentence\"]], [\"text\"]\n",
    ")\n",
    "\n",
    "result = pipeline.fit(df).transform(df)\n",
    "result.select(\"dependency.result\", \"entity.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "pipeline.write().overwrite().save(\"pipe_212\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pipeline in Scala 2.13\n",
    "The pipeline is now saved in a compatible format. Now we can set up a PySpark instance with Scala 2.13. \n",
    "\n",
    "Note that the session needs to be restarted and that we will need to reinstall PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download official spark archive and extract\n",
    "! wget https://archive.apache.org/dist/spark/spark-3.5.8/spark-3.5.8-bin-hadoop3-scala2.13.tgz\n",
    "! tar xzf spark-3.5.8-bin-hadoop3-scala2.13.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.4.4\n",
      "Uninstalling pyspark-3.4.4:\n",
      "  Successfully uninstalled pyspark-3.4.4\n",
      "Obtaining file:///content/spark-3.5.8-bin-hadoop3-scala2.13/python\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.8) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "  Running setup.py develop for pyspark\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyspark-3.5.8\n"
     ]
    }
   ],
   "source": [
    "# Install Scala 2.13 PySpark\n",
    "! pip uninstall -y pyspark\n",
    "! pip install -e /content/spark-3.5.8-bin-hadoop3-scala2.13/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start a custom Spark session to point to the right `spark-nlp` dependency. We need to replace the suffix `_2.12` to `_2.13`. So the coordinates will be `com.johnsnowlabs.nlp:spark-nlp_2.13:6.3.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d83ec092646b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b3ba41bcb60>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark NLP\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"16G\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.13:6.3.2\")\n",
    "    .getOrCreate()\n",
    ")  # note the package version ends with 2.13\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'version 2.13.8'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Scala version\n",
    "spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.load(\"pipe_212\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+--------------+\n",
      "|result                                                                           |result        |\n",
      "+---------------------------------------------------------------------------------+--------------+\n",
      "|[ROOT, Dependencies, represents, words, relationships, Sentence, Sentence, words]|[Dependencies]|\n",
      "+---------------------------------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [[\"Dependencies represents relationships betweens words in a Sentence\"]], [\"text\"]\n",
    ")\n",
    "\n",
    "result = pipeline.fit(df).transform(df)\n",
    "result.select(\"dependency.result\", \"entity.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have successfully exported a Scala 2.12 pipeline to Scala 2.13! You can upload it to the [Models Hub](https://nlp.johnsnowlabs.com/models) or use it directly on cloud platforms such as Databricks or Dataproc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
